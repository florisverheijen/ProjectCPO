{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will explain in a interactive way the tools you need for PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of the tutorial\n",
    "1. Introduction\n",
    "    * What is PCA?\n",
    "    * Application of PCA.\n",
    "    * Videos \n",
    "2. Step by step PCA\n",
    "    * Step 0: Importation and preparation of the dataset. \n",
    "    * Step 1: Standardization  \n",
    "    * Step 2: Calculation of the covariance matrix   \n",
    "    * Step 3: Calculation of the eigenvectors and eigenvalues\n",
    "    * Step 4: Determining the most important principal component \n",
    "    * Step 5: Calculation of the explained or overall variance\n",
    "    * Step 6: Cumulative plot of the principal components and scree plot\n",
    "    * Step 7: Reducing the dimensions \n",
    "    * Step 8: Plotting the data\n",
    "    * Step 9: Interpretation of the data \n",
    "2. PCA with Python\n",
    "    * Importation and preparation of the dataset. \n",
    "    * Standardization  \n",
    "    * Calculation of the covariance matrix   \n",
    "    * Calculation of the eigenvectors and eigenvalues\n",
    "    * Determining the most important principal component \n",
    "    * Calculation of the explained or overall variance\n",
    "    * Cumulative plot of the principal components and scree plot\n",
    "    * Reducing the dimensions \n",
    "    * Plotting the data\n",
    "    * Interpretation of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"height:10px;padding:0px;margin-bottom:-20px\"></div>\n",
    "\n",
    "## 1. Introduction \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"height:10px;padding:0px;margin-top:5px;\"></div>\n",
    "\n",
    "### 1.1 What is PCA?\n",
    "\n",
    "Nowadays, the size of the data has become a main bottleneck for the performance of many machine learning algorithms. Therefore, the data should be reduced before machine learning algorithms can perform well. Principal components analysis (PCA) is one of the solutions for this problem. \n",
    "PCA performs a linear transformation on the data, making a multidimensional data into a two or three dimensions data, which is easier to visualize. In such a manner, PCA is a dimensionality reduction method. Large datasets  have different variance values and the goal of PCA is to find the biggest variance values, because these retain most information when the data is reduce. The biggest variance will be expressed in the first principal component (PC 1). The second principal component contains the second largest variance, and so on. There are as many principal components as there are variables. However, only the PCs with the most variance will be used. These few principal components will be plot orthogonally (figure 1). That means the first principal component will be on the x axis and the second on the y axis.(Hereafter, there will be explained how it is possible to define the number of principal component and in which way you plot these data). The plot of the reduced dataset will contain as much as possible information about the original dataset. The plot will show the correlation between the most important variables of the original dataset. [Source1](https://www.sciencedirect.com/science/article/pii/B9780123838360000175) \n",
    "[Source2](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python) \n",
    " \n",
    "<img src=\"PCA3.JPG\" align=\"left\" height=\"250\" width=\"250\" /> <img src=\"PCA3.1.JPG\" height=\"250\" width=\"250\" /> <br>\n",
    "Figure 1: Visualization of PC1 and PC2.\n",
    "[Source3](https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/) <br>\n",
    "\n",
    "Note: Features, Dimensions, and Variables are all referring to the same thing. You will find them being used interchangeably. \n",
    "\n",
    "### 1.2 Application of PCA?\n",
    "\n",
    "*\tData visualization. In today’s world, the volume of the data and the variables that define the data are the mean challenges. You need extensive data exploration to find out how the variables are correlated or to understand the distribution of a few features. Considering a large dataset with many features/ variables.  Visualization can be a challenge and almost impossible. Hence, PCA could be have a great contribution to this problem. PCA could do this almost impossible visualization for you since it projects the large dataset into a lower dimension thereby allowing you to visualize the data in a 2D or 3D space with a naked eye.\n",
    "*\tSpeeding Machine Learning (ML) Algorithm. When you have to deal with a large dataset, it is possible that the ML algorithm’s learning become too slow. With PCA it is possible to speed up the machine learning testing considering the data has a lot of features since PCA reduce the dimension of the data. \n",
    "[Source4](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python)\n",
    "\n",
    "### 1.3 Videos\n",
    "For additional information, you can watch the following videos. \n",
    "\n",
    "* [Link to a YouTube video](https://www.youtube.com/watch?v=HMOI_lkzW08) (6 min)<br>\n",
    "* [Link to a YouTube video](https://www.youtube.com/watch?v=n7npKX5zIWI) (30 min) <br>\n",
    "* [Link to a YouTube video](https://www.youtube.com/watch?v=FgakZw6K1QQ&t=127s) (22 min) <br>\n",
    "* [LInk to a YouTube video](https://www.youtube.com/playlist?list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM) (pca videos 3,4,5,6,9,12)\n",
    "* [LInk to a YouTube video](https://www.youtube.com/watch?v=ZqXnPcyIAL8) (part 1,2 and 3)\n",
    "\n",
    "Very clear explanation of the formation of the covariance matrix <br>\n",
    "* [LInk to a YouTube video](https://www.youtube.com/watch?v=0GzMcUy7ZI0) (5 min)<br>\n",
    "\n",
    "Very clear explanation of eigenvalues, eigenvectors <br>\n",
    "* [LInk to a YouTube video](https://www.youtube.com/watch?v=HH8pouRwphA) (3 min)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"height:10px;padding:0px;margin-bottom:-20px\"></div>\n",
    "\n",
    "## 2. Step by step PCA \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"height:10px;padding:0px;margin-top:5px;\"></div>\n",
    "\n",
    "### Step 0: Importation and preparation of the dataset. \n",
    "\n",
    "This step is one of the most important steps during data analysis. The possibilities are enormous for importation and preparation of the dataset. In this course, the focus will be on the importation of the data with pandas. As well, the preparation will be done mostly with pandas' function. In this tutorial, this step is already done.\n",
    "\n",
    "### Step 1: Standardization. \n",
    "Standardization implies that the data would be scaled in a way that all the values of the variables lie within a similar range. This could be expressed with the following formula: \n",
    "Z = (variable value- mean)/ standard deviation \n",
    "  \n",
    "### Step 2: Calculation of the covariance matrix. <br>\n",
    "A little revision of the covariance matrix. The covariance matrix defines both the spread (variance), and the orientation (covariance) of the data. The variances are the diagonal elements and the covariances are the off-diagonal elements of the matrix. If the data is an N-dimensional data than the covariance matrix becomes an N x N matrix. For instance, the spread of a three-dimensional data will capture in a 3 x 3 covariance matrix . The covariance matrix S is defined by <br>\n",
    "$$S=\\frac{1}{N-1}BB^T$$ The computation of covariance matrix is necessary, because the values will be need in the following steps. [Source](https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)\n",
    "\n",
    "Additional information: <br>\n",
    "* [Link to website - Stat Trek](https://stattrek.com/matrix-algebra/covariance-matrix.aspx) \n",
    "* [Link to YouTube video -CompX: Mathematics of PCA - Covariance matrices](https://www.youtube.com/watch?v=0GzMcUy7ZI0)\n",
    " \n",
    "  \n",
    "### Step 3: Calculation of the eigenvectors and eigenvalues. \n",
    "Firstly, a little review of the definition of eigenvectors and eigenvalues. An eigenvector of a matrix A<sub>NxN</sub> is a nonzero vector x such that A$\\bullet$X = $\\lambda$X applies. An eigenvector is also a vector whose direction does not change when a linear transformation is performed.\n",
    "Eigenvalues ( $\\lambda$ in the formula) denote the magnitude of the respective eigenvectors. The eigenvalues are the values on the diagonal of the covariance matrix. The eigenvectors cannot be found directly in the covariance matrix. They must be calculated. The calculation of the eigenvalues and eigenvectors is important, because the eigenvector with the largest respective eigenvalue contains the most variance (most information of the data). The eigenvectors with the largest eigenvalue is called principal component 1 (PC1). PCs are the new set of variables that are obtained from the initial set of variables. They compress and possess most of the useful information that was scattered among the initial variables.\n",
    "\n",
    "Additional information:\n",
    "* [Link to YouTube video- CompX: Mathematics of PCA - Matrices](https://www.youtube.com/watch?v=HH8pouRwphA)\n",
    "* [Link to YouTube video- PCA 5: finding eigenvalues and eigenvectors](https://www.youtube.com/watch?v=2fCBE7DWgd0&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM&index=6&t=6s)\n",
    "\n",
    "  \n",
    "### Step 4: Determining the most important principal component.  \n",
    "In the previous step is explained that the eigenvectors with the largest eigenvalues contain the most information. Hence, it is important to find these values. Is this step the eigenvalues and eigenvectors will be listed in tuples. Thereafter, the list of tuples will be sort and put in the reverse order so that the biggest eigenvalues will be at the top of the list. \n",
    "  \n",
    "### Step 5: Calculation of the explained or overall variance. \n",
    "How many principal components do you need to get a new dataset with as much as possible information and the fewest dimensions? A useful measure is the ‘explained variance’, which can be calculated from the eigenvalues. The explained variance determines how much information (variance) can be attributed to each of the principal components. This will be done by calculating the sum of all the eigenvalues. Next, for each eigenvalue will be calculated for how much percentage they contribute to the sum of all the eigenvalues.  \n",
    "  \n",
    "### Step 6: Cumulative plot of the principal components and a scree plot.\n",
    "Principal components are created in order of the amount of variation they cover: PC1 captures the most variation, PC2 — the second most, and so on. Each of them contributes some information of the data. Leaving out PCs and we lose information. The good news is, if the first two or three PCs have capture most of the information, then we can ignore the rest without losing anything important.\n",
    "With a cumulative plot, it will be easier to see how much principal component are needed. When the cumulative plot is above 70 %, the principal components that form this 70 % give a good enough picture of the entire dataset. [Source5](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/#visualization-and-interpretation) The cumulative plot will be made with the percentage of the eigenvalues which are calculated in the previous step. This plot will contain a step and bar plot. The bar plot makes it clear which principal component is the largest. With the step plot it is easy to see when the selected principal components contain enough information so that the decision can be made that the principal components contain enough information (figure 2). <br>\n",
    "Another option to visualization option is a scree plot. A scree plot displays how much variation each principal component captures from the data. The y axis is eigenvalues, which essentially stand for the amount of variation. An ideal curve should be steep, then bends at an “elbow” — this is your cutting-off point — and after that flattens out. In Figure 3, just PC 1,2, and 3 are enough to describe the data.\n",
    "\n",
    "To deal with a not-so-ideal scree plot curve, there are a couple ways: (This will not be covered in this course)\n",
    "* Kaiser rule: pick PCs with eigenvalues of at least 1.\n",
    "* Proportion of variance plot: the selected PCs should be able to describe at least 80% of the variance.\n",
    "If you end up with too many principal components (more than 3), PCA might not be the best way to visualize your data. Instead, consider other dimension reduction techniques, such as t-SNE and MDS. [Source6](https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/)\n",
    "\n",
    "<img src=\"PCA2.JPG\" align=\"left\" height=\"250\" width=\"250\" /> <img src=\"PCA4.JPG\" height=\"400\" width=\"400\" /> <br>\n",
    "Figure 2: An example of a cummulative plot of the principal components. [Source7](https://hackernoon.com/creating-visualizations-to-better-understand-your-data-and-models-part-1-a51e7e5af9c0)  <span style=\"color:white\">word</span> Figure 3: PCA scree plot. [Source8](https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/) <br>\n",
    "  \n",
    "### Step 7: Reducing the dimensions.\n",
    "The number of the principal component that have been selected in the previous step will be put in a new matrix named projection matrix (W). The inner product of the projection matrix and the standardized matrix will be calculated. With this inner product, the original data will be plotted on the new subspace. In other words, a linear transformation is performed. \n",
    "  \n",
    "#### Step 8: Plotting the data. \n",
    "In this last step, you plot the transformed data in a scatterplot.PC1 will always be on the x axis, PC2 on the y-as and so on. The data may contain labels. In this tutorial, the dataset contains the labels Benign and Malignant. These labels get a different color so that it becomes visual how the features of these labels difference.\n",
    "\n",
    "### Step 9: Interpretation of the data.\n",
    "The vizualation will be explained through an example with a dataset named iris dataset. The dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. If PCA is applied on the iris dataset, the plot in figure 4 will be obtained. It could be seen that the four dimenions are reduced to two dimensions (PC1 and PC2). This is done because PC1 and PC2 contain almost 100% of the variance. <br> <br>\n",
    "The important question is now, how to interpret this plot? <br> <br>\n",
    "There are three labels (Iris setosa, Iris virginica and Iris versicolor). In an ideal PCA plot you will observe that these labels had formed two separated clusters (groups). Because the samples associated with a particular label have roughly the same properties. If the clusters are different based on PC1 (they different in location on the x axis), differences are likely to be due to the variables that have heavy influences on PC1. If the clusters are different based on PC2 then the variables that heavily influences PC2 are likely to be responsible for the difference.  \n",
    "The PCs are ranked by how much variance they contain. PC1 reveals the most variation, while PC2 reveals the second most variation. Therefore, differences among clusters along PC1 axis are actually larger than the similar-looking distances along PC1 axis. <br> <br>\n",
    "The second important question is, which variables influences PC1 the most and which PC2? <br> <br>\n",
    "This question could be answered with plotting a loading plot. A loading plot shows how strongly each variable influences a principal component. The loading plot for the first two PCs of the iris dataset could be seen in figure 5. See how these vectors are pinned at the origin of PCs (PC1 = 0 and PC2 = 0)? Their project values on each PC show how much weight they have on that PC. In this example, petal_len, petal_wid and sepal_len strongly influence PC1, while sepal_wid have more say in PC2. <br>\n",
    "\n",
    "Another nice thing about loading plots: the angles between the vectors tell us how characteristics correlate with one another. \n",
    "* When two vectors are close, forming a small angle, the two variables they represent are positively correlated. \n",
    "* If they meet each other at 90°, they are not likely to be correlated.\n",
    "* When they diverge and form a large angle (close to 180°), they are negative correlated. \n",
    "\n",
    "From these two plot can be concluded that the three clusters are different based on PC1 due to the variables petal_len, petal_wid and sepal_len. Because of the fact that these variables influence PC1 the most.\n",
    "\n",
    "<img src=\"PCA6.JPG\" align=\"left\" height=\"400\" width=\"400\" /> <img src=\"PCA7.JPG\" height=\"400\" width=\"400\" /> <br>\n",
    "Figure 2: Loading plot. <span style=\"color:white\">word............................................................................................</span> Figure 3: PCA plot\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"height:10px;padding:0px;margin-bottom:-20px\"></div>\n",
    "\n",
    "## 3. PCA with Python \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"height:10px;padding:0px;margin-top:5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importation and preparation of the dataset.\n",
    "\n",
    "In this first tutorial you will work with a dataset about breast cancer. The Breast Cancer data set is a real-valued multivariate data that consists of two classes, where each class signifies whether a patient has breast cancer or not. The two categories are: malignant and benign. The malignant class has 212 samples, whereas the benign class has 357 samples.\n",
    "It has 30 features shared across all classes: radius, texture, perimeter, area, smoothness, fractal dimension, etc. \n",
    "\n",
    "In this tutorial the codes for the importation and preparation of the code are already given. In this section, you only have to run the code so that you can see what is happen. Furthermore, the cells have to be run before you can start with the next secion\n",
    "\n",
    "The dataset will be imported with the following code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.base.load_breast_cancer>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*load_breast_cancer* will give you the data and the labels ( malignant or benign). With *.data* you will get the data, with *.target* you wil get the labels and with *feature_names* you will get the features that are in the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breast_data = load_breast_cancer().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breast_labels = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension'], \n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = load_breast_cancer().feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the import of numpy since you will reshaping the *breast_labels* to concatenate it with the *breast_data* so that you can finally create a *DataFrame* with Pandas which will have both the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.reshape(breast_labels,(569,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.79900000e+01,   1.03800000e+01,   1.22800000e+02, ...,\n",
       "          4.60100000e-01,   1.18900000e-01,   0.00000000e+00],\n",
       "       [  2.05700000e+01,   1.77700000e+01,   1.32900000e+02, ...,\n",
       "          2.75000000e-01,   8.90200000e-02,   0.00000000e+00],\n",
       "       [  1.96900000e+01,   2.12500000e+01,   1.30000000e+02, ...,\n",
       "          3.61300000e-01,   8.75800000e-02,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  1.66000000e+01,   2.80800000e+01,   1.08300000e+02, ...,\n",
       "          2.21800000e-01,   7.82000000e-02,   0.00000000e+00],\n",
       "       [  2.06000000e+01,   2.93300000e+01,   1.40100000e+02, ...,\n",
       "          4.08700000e-01,   1.24000000e-01,   0.00000000e+00],\n",
       "       [  7.76000000e+00,   2.45400000e+01,   4.79200000e+01, ...,\n",
       "          2.87100000e-01,   7.03900000e-02,   1.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_breast_data = np.concatenate([breast_data,labels], axis=1)\n",
    "final_breast_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to convert the array to a DataFrame with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9  ...      21      22      23      24      25      26      27  \\\n",
       "0  0.07871 ...   17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667 ...   23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999 ...   25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744 ...   26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883 ...   16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29   30  \n",
       "0  0.4601  0.11890  0.0  \n",
       "1  0.2750  0.08902  0.0  \n",
       "2  0.3613  0.08758  0.0  \n",
       "3  0.6638  0.17300  0.0  \n",
       "4  0.2364  0.07678  0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_dataset = pd.DataFrame(final_breast_data)\n",
    "breast_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be seen that the features are missing in this Dataset. So, in de next steps we are going to append these features to the dataset. \n",
    "If you note in the feature array, the label field is missing. Hence, you will have to manually add it to the feature array since you wil be equating this array with the column names of your *breast_dataset* dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_labels = np.append(features,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breast_dataset.columns = features_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...    worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...            17.33           184.60      2019.0   \n",
       "1                 0.05667  ...            23.41           158.80      1956.0   \n",
       "2                 0.05999  ...            25.53           152.50      1709.0   \n",
       "3                 0.09744  ...            26.50            98.87       567.7   \n",
       "4                 0.05883  ...            16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  label  \n",
       "0          0.4601                  0.11890    0.0  \n",
       "1          0.2750                  0.08902    0.0  \n",
       "2          0.3613                  0.08758    0.0  \n",
       "3          0.6638                  0.17300    0.0  \n",
       "4          0.2364                  0.07678    0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original labels are in 0,1 format, you will change the labels to benign and malignant using *.replace* function. You will use *inplace=True* which will modify the dataframe breast_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension   ...    worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871   ...            17.33           184.60      2019.0   \n",
       "1                 0.05667   ...            23.41           158.80      1956.0   \n",
       "2                 0.05999   ...            25.53           152.50      1709.0   \n",
       "3                 0.09744   ...            26.50            98.87       567.7   \n",
       "4                 0.05883   ...            16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension   label  \n",
       "0          0.4601                  0.11890  Benign  \n",
       "1          0.2750                  0.08902  Benign  \n",
       "2          0.3613                  0.08758  Benign  \n",
       "3          0.6638                  0.17300  Benign  \n",
       "4          0.2364                  0.07678  Benign  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_dataset['label'].replace(0, 'Benign',inplace=True)\n",
    "breast_dataset['label'].replace(1, 'Malignant',inplace=True)\n",
    "breast_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now the dataset is ready to get started with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Standarization \n",
    "\n",
    "The first step by PCA is always standardizing the data since PCA's output is influenced based on the scale of the features of the data. \n",
    "To apply normalization, you will import *StandardScaler* module from the sklearn library and select only the features from the *breast_cancer* (with *.loc* and *.values* to get only the values of the selected features) you created in the first part of this paragraph. So you select all the data expect the data in the colomns 'label'. Once you have the features, you will then apply scalling by doing *fit_transform* on the feature data. \n",
    "\n",
    "\n",
    "### Excercise 3.2.1\n",
    "\n",
    "Use the command code *StandardScaler* and *fit_transform* to write a code that can calculate the standardization of the whole data, except the colomn lable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.2.2\n",
    "\n",
    "Let's check whether the normalized data has a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.2.3\n",
    "\n",
    "Write below a code that shows the new data table with the normalizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculation of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.3.1\n",
    "\n",
    "Calculate the Covariance-matrix using the command *cov*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Calculation of the Eigenvalues and Eigenvectoren\n",
    "\n",
    "### Excercise 3.4.1\n",
    "\n",
    "Calculate the Eigenvalues and Eigenvectoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code.....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Determining the most important principal component\n",
    "\n",
    "In order to decide which eigenvector(s) can dropped without losing too much information for the construction \n",
    "of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: \n",
    "The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; \n",
    "those are the ones that can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top k eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.5.1\n",
    "\n",
    "Make a eig_pairs list of (eigenvalue, eigenvector) tulples and print the Eigenvalues in descending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Calculation of the explained or overall variance\n",
    "After sorting the eigenpairs, the next question is \n",
    "“how many principal components are we going to choose for our new feature subspace?” \n",
    "A useful measure is the so-called “explained variance,” which can be calculated from the eigenvalues. \n",
    "The explained variance tells us how much information (variance) can be attributed to each of the principal components\n",
    "\n",
    "### Excercise 3.6.1\n",
    "\n",
    "Calculate the totale varaince of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Excercise 3.6.2\n",
    "\n",
    "Calculate the explained variance (var_exp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.6.3\n",
    "\n",
    "Calculate the Cummulative explained variance (cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Cummulative plot of the principal components and scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.7.1\n",
    "\n",
    "Make in one plot a bar graph of the individual explained variance and a step graph of the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to get a visualization of the contribution of each eigenvalue is a scree plot. A scree plot is a line plot which plot the eigenvalues in descending order. \n",
    "\n",
    "### Excercise 3.7.2\n",
    "\n",
    "Make a scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Reducing the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cummulative plot of the principal components and the scree plot, it can be concluded that the first two principal components contain enough information so that the dataset can be reduced to a 2D-plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.8.1\n",
    "\n",
    "Reshape the eig_pair data in matrix_w with only the Eigenvectors of the two highst eigenvalues. *hint: use hstack*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.8.2\n",
    "\n",
    "Calculate the inner product of X and matrix_w and name it **Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.9.1\n",
    "\n",
    "Make a scatter plot that display the data of bennign in red and maligant in green with the plottitle *Principal Component Analysis of Breast Cancer Dataset*,<br> x axis Principal Component 1, y axis Principal Component 2 and legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Interpretation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.10.1\n",
    "\n",
    "Give a interpretation of the data that is present in the scatter plot in exercise 11. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type here your answer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
